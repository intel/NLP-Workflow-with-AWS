{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7c854e",
   "metadata": {},
   "source": [
    "Copyright (C) 2022 Intel Corporation\n",
    " \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    " \n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions\n",
    "and limitations under the License.\n",
    " \n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c259928",
   "metadata": {},
   "source": [
    "# General Description\n",
    "\n",
    "Version: 1.1 Date: Oct 28, 2022\n",
    "\n",
    "This notebook outlines the general usage of cloud inference platform using Intel's CPU, quantized model by Intel Neural Compressor on Amazon SageMaker platform. This illustrate how users can use REST api to send a request to the endpoint and get the model output.\n",
    "\n",
    "Users may wish to based on parts of the codes and customize those to suit their purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b5d22",
   "metadata": {},
   "source": [
    "# Step 0: Specify the AWS information (Optional)\n",
    "\n",
    "Users may wish to specify the AWS information in the ./config/config.yaml file to pre-fill the necessary information required for the workflow. Or users may also fill in the necessary fields when executing the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a414a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./config/sagemaker_config.yaml') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "    read_from_yaml = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371c829",
   "metadata": {},
   "source": [
    "# Step 1: Build a custom docker image for inference\n",
    "    1. Copy the content of the \"../src/sagemaker_inference_container\" and paste those outside the docker container. \n",
    "    2. Modify the AWS credential of the build_and_push.sh \n",
    "       Pay attention to the region, account number, algorithm_name and the firewall issue \n",
    "    3. Run build_and_push.sh to build the custom docker image for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bef67",
   "metadata": {},
   "source": [
    "# Step 2: Deploy the model using SageMaker \n",
    "\n",
    "Users may wish to change the type of the cluster nodes and the number of the nodes for the serving \n",
    "\n",
    "Please change the two variables 'deploy_instance_type' and 'num_of_nodes' to achieve this purpose.\n",
    "\n",
    "List of EC2 instances: https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bfa43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "if read_from_yaml:\n",
    "    sagemaker_role = config_dict['role']\n",
    "    inference_image = config_dict['inference_image_uri']\n",
    "    model_data = config_dict['quantized_model_s3_path']\n",
    "else:\n",
    "    sagemaker_role = '' # AmazonSageMaker-ExecutionRole-xxxxxxxxxxxxxx\n",
    "    inference_image = '' #e.g.: xxxxxxxxxx.dkr.ecr.us-west-1.amazonaws.com/sagemaker-inteltf-huggingface-inc-inference\n",
    "    model_data = '' #The quantized model trained in the 1.0-intel-sagemaker-training.ipynb - e.g.: s3://sagemaker-us-west-1-xxxxxxxxx/model/model.tar.gz\n",
    "\n",
    "#Specify the type of target nodes and number of nodes for the deployment\n",
    "deploy_instance_type = \"ml.c5.xlarge\" #default value\n",
    "num_of_nodes=1                        #default value\n",
    "\n",
    "model = TensorFlowModel(model_data=model_data, role=sagemaker_role, image_uri=inference_image)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=num_of_nodes, instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e27553",
   "metadata": {},
   "source": [
    "# Step 3: Preprocess the input data and send it to the endpoint\n",
    "The tokenizer has already download offline and put into the container. The choice of it depends on the type of model and the task. Users may feel free to change it or use another pre-process method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff06fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained('../src/sagemaker_inference_container/bert_uncased_tokenizer') #model dependent. User may switch to the one that suit for their use case.\n",
    "sentence1 = 'Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers.'\n",
    "sentence2 = 'Sheena Young, a spokesman for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.'\n",
    "processed_input = tokenizer(sentence1, sentence2, padding=True, truncation=True)\n",
    "batch = [dict(processed_input)]\n",
    "input_data = {\"instances\": batch}\n",
    "\n",
    "#Use JSON format to call the prediction\n",
    "result = predictor.predict(input_data)\n",
    "prediction = np.argmax(result['predictions'][0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829a592",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
