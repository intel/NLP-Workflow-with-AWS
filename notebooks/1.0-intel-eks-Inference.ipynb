{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2898045f",
   "metadata": {},
   "source": [
    "Copyright (C) 2022 Intel Corporation\n",
    " \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    " \n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions\n",
    "and limitations under the License.\n",
    " \n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898431",
   "metadata": {},
   "source": [
    "# General Description\n",
    "\n",
    "Version: 1.0\n",
    "Date: Sep 16, 2022\n",
    "\n",
    "This notebook outlines the general usage of quantizing a FP32 HuggingFace model to INT8 and inferencing using Intel Neural Compressor and Intel's latest CPU (with VNNI instruction support).\n",
    "\n",
    "A quantiezed model will be stored into a Amazon Elastic File System.\n",
    "\n",
    "Users are free to based on any part of the codes and customize those to suit their purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50776f4b",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "1. It is expected that there is already a EKS cluster hosted and setup properly. \n",
    "   The EKS cluster should have:\n",
    "   i.   Kubeflow installed\n",
    "   ii.  Associated related AWS credentials (i.e. you are able to use 'kubectl' to create/delete training jobs)\n",
    "   iii. A Elastic File System (EFS) that is accesible(read and write) by the cluster nodes\n",
    "   iv.  2 nodes or above (optional but recommended)\n",
    "   \n",
    "   Please contact your EKS administrator to obtain the necessary information.\n",
    "   \n",
    "   You may also refer to the following webpage to create a new EKS cluster and install the kubeflow.\n",
    "   - https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-eks-setup.html\n",
    "   - https://www.eksworkshop.com/advanced/420_kubeflow/install/\n",
    "   \n",
    "2. Setup the Amazon credential credential (e.g.: aws configure) \n",
    "   i.  in the container\n",
    "   ii. the docker host \n",
    "\n",
    "3. Set the notebook kernel to use 'Python 3 (ipykernel)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3a628",
   "metadata": {},
   "source": [
    "# Step 1: Build a custom docker image for inference\n",
    "\n",
    "    1. Copy the content of the \"../src/eks_inference_container\" and paste those outside the docker container. \n",
    "    2. Modify the AWS credential of the build_and_push.sh \n",
    "       Pay attention to the region, account number, algorithm_name and the firewall issue\n",
    "    3. Run build_and_push.sh to build the custom docker image for inference.\n",
    "\n",
    "Note: \n",
    "- Users may change the content of the \"inc_quantization.py\" to adjust the output of the quantized model (into EFS) and the inc_config.yaml to change the quantization type/distillation.\n",
    "- For this reference design, it is assumed that the EFS mounted path is /data and accessible by the cluster nodes. The previously trained model is stored under /data. Intel Neural Compressor will read the FP32 model and quantize it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028071f",
   "metadata": {},
   "source": [
    "# Step 2a: Set kubectl to the target cluster\n",
    "\n",
    "We will first use the following command to set `kubectl`.\n",
    "\n",
    "Please modify and run the follwoing command to set the proper cluster target for 'kubectl'\n",
    "<region> is the Amaozn EKS Region\n",
    "<cluster-name> is the cluster name shown in the Amazon Elastic Kubernetes Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws eks --region <region> update-kubeconfig --name <cluster-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b982ce",
   "metadata": {},
   "source": [
    "# Step 2b: Get the details of the nodes\n",
    "\n",
    "The following command will list the nodes we are able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "kubectl get nodes -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83813a2",
   "metadata": {},
   "source": [
    "# Step 2c: Label the nodes\n",
    "\n",
    "For the quantization, you may just need to specify a node for the quantization. Please label the one that has highest computational power as quantization may take some time.\n",
    "You may run the following command several times to label the node manually. Please modify the content in the '<>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "kubectl label nodes <name-of-the-node> nodename=<node1> --overwrite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d5f42",
   "metadata": {},
   "source": [
    "# Step 3: Assign the training job by modifying the config file (.yaml)\n",
    "\n",
    "The quantization job is created by calling 'kubectl' on the config file (distributed_1node_inc.yaml). By modifying the config file, we can assign the computational resources (i.e. the nodes and EFS) for the training job.\n",
    "\n",
    "Please modify the distributed_training.yaml to specify the resources, especially the following items:\n",
    "1. name        -- change to a name that suits the task. This will be used when checking the logs/pods execution.\n",
    "2. replicas    -- it is recommended to set 1 for the Master and n-1 (n is the total number of nodes) for worker \n",
    "3. image       -- it should be the one you built in step 1\n",
    "4. mountPath   -- the path where the fine-tuned model is stored\n",
    "5. claimName   -- the name of the EFS storage. May need to ask EKS administrator regarding to the details\n",
    "6. cpu         -- it is recommended to set the value equals to (n/2) + 1, where n is the number of vCPU of the instance. This \n",
    "                  will be a hint to inform Kubernate to distribute the training job evenly to the computational nodes. \n",
    "                  Otherwise the job may stay in 1 node and result in poor performance (i.e. not the expected distributed \n",
    "                  training)\n",
    "7. memory      -- you may check the type of the instance and request depends on the task nature\n",
    "8. values      -- use the labels set in Step 2c. This tells K8S to use specific nodes for training and assign correct affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9a3ad",
   "metadata": {},
   "source": [
    "# Step 4: Start the quantiztion\n",
    "\n",
    "After having the pod specification defined in the previous step, users may start the training job on the EKS cluster by calling the command in the next block cell.\n",
    "\n",
    "Users may use the following command to check the execution status/logs of the training job\n",
    "1. kubectl logs -f <name-of-the-job/pod>\n",
    "2. kubectl describe pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0535a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "kubectl create -f ../src/eks_inference_container/distributed_1node_inc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8c0c1",
   "metadata": {},
   "source": [
    "# Step 5: Performing inference using the quantized model\n",
    "\n",
    "Once the model has been quantized, users may deploy the quantized model.\n",
    "The following code block is a simple example to demonstrate how to perform data preprocessing and retrieve the result.\n",
    "\n",
    "Please duplicate the code and put into a EKS node (with access to the EFS) to experiment the results.\n",
    "For the package dependency, please refer to the Dockerfile (in inference_container/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e290ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020-2022 Intel Corporation.\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import argparse, os\n",
    "from urllib.parse import uses_fragment\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BertTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AdamW\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #P assing in environment variables and hyperparameters \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--quantized_model_path\", type=str, default='/data/quantized_model')\n",
    "    parser.add_argument(\"--max_seq_length\", type=int, default='128')\n",
    "    parser.add_argument(\"--model_name\", type=str, default='bert-base-uncased')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    from neural_compressor.utils.load_huggingface import OptimizedModel\n",
    "    model = OptimizedModel.from_pretrained(\n",
    "            args.quantized_model_path\n",
    "        )\n",
    "    \n",
    "    # Load the MRPC dataset\n",
    "    # It can be a single input using dictionary to contain\n",
    "    train_dataset, test_dataset = load_dataset('glue', 'mrpc', split=[\"train\", \"test\"])\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, truncation=True, padding=\"max_length\", max_length=args.max_seq_length)\n",
    "    \n",
    "    correct_pred = 0\n",
    "    # Inference on every single input\n",
    "    for i in range(len(train_dataset['sentence1'])):\n",
    "        # Preprocess the input using the tokenizer\n",
    "        sentence1 = train_dataset['sentence1'][i] #string type\n",
    "        sentence2 = train_dataset['sentence2'][i] #string type\n",
    "        example_inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\")\n",
    "        \n",
    "        # Perform inference\n",
    "        results = model(**example_inputs)\n",
    "        probability = F.softmax(results['logits'][0], dim=0)\n",
    "        pred = torch.argmax(probability).item()\n",
    "\n",
    "        gt = train_dataset['label'][i]\n",
    "        if pred == gt:\n",
    "            correct_pred = correct_pred + 1\n",
    "   \n",
    "    print('Accuracy: ' + str(correct_pred/len(train_dataset['sentence1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb82ee7",
   "metadata": {},
   "source": [
    "# Step 6: Clean up\n",
    "\n",
    "After the quantization is completed, the INT8 model should be stored in the EFS mountPath (in this case, /data). Users may wish to release the resources by calling the following two commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "kubectl delete <pytorchjobs.kubeflow.org/the-job-name> #e.g.: pytorchjobs.kubeflow.org/xxxxxxxxxxxxx\n",
    "kubectl delete pod --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
